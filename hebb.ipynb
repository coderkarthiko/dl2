{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code illustrates the fast AI implementation of the unsupervised \"biological\" learning algorithm from [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116) on MNIST data set. \n",
    "If you want to learn more about this work you can also check out this [lecture](https://www.youtube.com/watch?v=4lY-oAY0aQU) from MIT's [6.S191 course](http://introtodeeplearning.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell loads the data and normalizes it to the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "\n",
    "Nc = 10\n",
    "N = 784\n",
    "Ns = 60000\n",
    "M = np.zeros((0, N))\n",
    "for i in range(Nc):\n",
    "    M = np.concatenate((M, mat['train'+str(i)]), axis=0)\n",
    "M = M / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a heatmap of the weights a helper function is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(synapses, Kx, Ky):\n",
    "    yy = 0\n",
    "    HM = np.zeros((28 * Ky, 28 * Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y * 28:(y + 1) * 28, x * 28:(x + 1) * 28] = synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc = np.amax(np.absolute(HM))\n",
    "    im = plt.imshow(HM, cmap='bwr', vmin=-nc, vmax=nc)\n",
    "    fig.colorbar(im, ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines paramaters of the algorithm: `eps0` - initial learning rate that is linearly annealed during training; `hid` - number of hidden units that are displayed as an `Ky` by `Kx` array by the helper function defined above; `mu` - the mean of the gaussian distribution that initializes the weights; `sigma` - the standard deviation of that gaussian; `Nep` - number of epochs; `Num` - size of the minibatch; `prec` - parameter that controls numerical precision of the weight updates; `delta` - the strength of the anti-hebbian learning; `p` - Lebesgue norm of the weights; `k` - ranking parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps0 = 2e-2    # learning rate\n",
    "Kx = 10\n",
    "Ky = 10\n",
    "hid = Kx * Ky    # number of hidden units that are displayed in Ky by Kx array\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "Nep = 200      # number of epochs\n",
    "Num = 100      # size of the minibatch\n",
    "prec = 1e-30\n",
    "delta = 0.4    # Strength of the anti-hebbian learning\n",
    "p = 2.0        # Lebesgue norm of the weights\n",
    "k = 2          # ranking parameter, must be integer that is bigger or equal than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the main code. The external loop runs over epochs `nep`, the internal loop runs over minibatches. For every minibatch the overlap with the data `tot_input` is calculated for each data point and each hidden unit. The sorted strengths of the activations are stored in `y`. The variable `yl` stores the activations of the post synaptic cells - it is denoted by g(Q) in Eq 3 of [Unsupervised Learning by Competing Hidden Units](https://doi.org/10.1073/pnas.1820458116), see also Eq 9 and Eq 10. The variable `ds` is the right hand side of Eq 3. The weights are updated after each minibatch in a way so that the largest update is equal to the learning rate `eps` at that epoch. The weights are displayed by the helper function after each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(12.9,10))\n",
    "\n",
    "synapses = np.random.normal(mu, sigma, (hid, N))\n",
    "for nep in range(Nep):\n",
    "    eps = eps0 * (1 - nep / Nep)\n",
    "    M = M[np.random.permutation(Ns),:]\n",
    "    for i in range(Ns // Num):\n",
    "        inputs = np.transpose(M[i * Num:(i+1) * Num,:])\n",
    "        sig = np.sign(synapses)\n",
    "        tot_input = np.dot(sig * np.absolute(synapses) ** (p - 1), inputs)\n",
    "        \n",
    "        y = np.argsort(tot_input, axis=0)\n",
    "        yl = np.zeros((hid, Num))\n",
    "        yl[y[hid - 1,:], np.arange(Num)] = 1.0\n",
    "        yl[y[hid - k], np.arange(Num)] = -delta\n",
    "        \n",
    "        xx = np.sum(np.multiply(yl, tot_input), 1)\n",
    "        ds = np.dot(yl, np.transpose(inputs)) - np.multiply(np.tile(xx.reshape(xx.shape[0],1), (1,N)), synapses)\n",
    "        \n",
    "        nc = np.amax(np.absolute(ds))\n",
    "        if nc < prec:\n",
    "            nc = prec\n",
    "        synapses += eps * np.true_divide(ds,nc)\n",
    "        \n",
    "    draw_weights(synapses, Kx, Ky)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
